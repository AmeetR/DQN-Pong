{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdee59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from wrapppers.atari_wrappers import *\n",
    "from utils import get_state, select_action\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import Video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e9952",
   "metadata": {},
   "source": [
    "# What is this notebook for? \n",
    "\n",
    "This notebook is intended to walk through how the network interacts with a test environment and run an episode with a \n",
    "pretrained network. It assumes that you've trained a network using the training code in this repo or you have a copy of the trained weights for this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32481d",
   "metadata": {},
   "source": [
    "Let's start by defining a named tuple that denotes an observation. This helps us provide a convenient data object for storing observations in our Replay Memory. We need to store the `state`, `action`, `next state`, and `reward` in order to train our model later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f5d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',  ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a5da9",
   "metadata": {},
   "source": [
    "# DQN Network\n",
    "\n",
    "Below, we define the DQN neural network. This is not strictly necessary for the testing code, as the network is reconstructed from the weights. Our network consists of three convolutional layers and two linear layers, as defined below. The network takes in a state ($s$) as input and outputs a vector of $Q(s,a)$ for all possible actions $a$ from that state. $Q(s,a)$ defines the quality function of a given state-action pair. The network is then trained on real quality actions from the received reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2adcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements DQN with conv layers. The idea is to take an impage of the Pong\n",
    "    game state as input and output a vector of \n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels in the input image\n",
    "        n_actions (int): number of possible actions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels = 4, n_actions = 4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208e557",
   "metadata": {},
   "source": [
    "# Memory Replay\n",
    "\n",
    "We define the `ReplayMemory` data structure below. This is strictly to allow us to have a friendly way to deal with the `ReplayMemory`. It's functionally, just a list with a sampling function build in. It allows us to have a nice way to store our observations (with the transition named tuple above) and sample from it when we need to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac9e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3a99d",
   "metadata": {},
   "source": [
    "# Testing Function\n",
    "\n",
    "Here we define our `test` function. This takes in a Gym environment (`env`), a number of episodes to test on (`n_episodes`), and a `render` boolean. The gym environment is what the agent interacts with. In our case, it's an environment for Pong, but Gym has environments for hundreds of games, including robot applications and others. Gym allows our focus to be on the RL algorithm instead of having to worry about physics. \n",
    "\n",
    "## Monitor\n",
    "In the first line of the function, we define `gym.wrappers.Monitor`. This is a wrapper around our Gym environment that allows us to track some metadata about the exectued environment and it will save a video to the file path that we passed in. The `force` parameter decides whether we want to rewrite data in a folder that already has Gym wrapper data in it. The agent can interact with the wrapper in the same way that it could with the raw environment. \n",
    "\n",
    "\n",
    "## Testing loop\n",
    "\n",
    "We have two loops. The first loop iterates through the number of episodes and the second iterates through an episode. In the case of Pong, an episode is just the length of the game. More generally, an episode is from a start state to an end state, so in any game, the start state is the start of the game and the end state is the end of the game. \n",
    "\n",
    "Before each episode, we do the following:\n",
    "1. Reset the environment \n",
    "2. Get a start state\n",
    "3. Reset reward attained in a given episode to zero. \n",
    "\n",
    "During each episode, we do the following: \n",
    "1. Predict a new action by selecting the action that gives us the maximum Q-value from our network. \n",
    "2. Render the environment if that argument is True\n",
    "3. Tell the environment what our agent's predicted action is and receive information about observation, reward, and whether the game ended. \n",
    "4. If the game hasn't ended, get the next state. Otherwise, report the total reward attained during that episode and go to the next episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46057fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, n_episodes, render=True):\n",
    "\n",
    "    env = gym.wrappers.Monitor(env, SAVE_DIR + 'dqn_pong_video', force = True)\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        state = get_state(obs)\n",
    "        total_reward = 0.0\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state.cuda()).max(1)[1].view(1, 1)\n",
    "            if render:\n",
    "                env.render() \n",
    "                \n",
    "                time.sleep(0.02)\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if not done:\n",
    "                next_state = get_state(obs)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print(\"Finished Episode {} with reward {}\".format(episode, total_reward))\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc166e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = make_env(env)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights_path = 'output/dqn_pong_model.pth'\n",
    "SAVE_DIR = './videos/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961792e",
   "metadata": {},
   "source": [
    "# Loading the Network\n",
    "\n",
    "We use `torch.load` to load the network from the saved weights path. Calling `.to(device)` either passes the network to the GPU or the CPU, depending on if there's a Nvidia GPU in the system (and PyTorch CUDA is initialized correctly). \n",
    "\n",
    "During training, we use `torch.save` to save the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd989f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "policy_net = torch.load(weights_path).to(device)\n",
    "print(policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e84e3d",
   "metadata": {},
   "source": [
    "Now, we simply have to call the function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69805c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameet\\Anaconda3\\envs\\torch_gym\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\ameet\\Anaconda3\\envs\\torch_gym\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Episode 0 with reward 21.0\n"
     ]
    }
   ],
   "source": [
    "test(env, 1, render = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57122a7",
   "metadata": {},
   "source": [
    "We can replay the video of the agent, as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c4d960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,./videos/dqn_pong_video/openaigym.video.1.24628.video000000.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(SAVE_DIR + 'dqn_pong_video/openaigym.video.1.24628.video000000.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053dda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
